# 大模型结构

## 传统模型 vs 大语言模型

- 传统模型：vec2vec，一个输入对应一个输出，即输出长度与输入长度一致；
- 大语言模型：seq2seq，输出与输入都是任意长度的序列，长度不一致；

## 从文本到向量输入

### 词表生成（BPE算法）

1. 初始化：将文本中每个字符当作一个独立的 token；
2. 统计频率：计算文本中每个字符对的出现频率；
3. 合并字符对：找到频率最高的字符对，并将其合并为一个新的 token；
4. 更新词汇表：将新生成的 token 添加到词汇表中，并在文本中替换所有该字符对出现的地方；
5. 重复步骤 2-4：反复执行上述步骤，直到达到预定的 token 数量或不能再找到新的字符对；

### Tokenizer

1. Encode：将文本转换为 token 序列；
2. Decode：将 token 序列转换为文本；

### 词嵌入（Embedding）

#### one-hot 编码

- 将每个 token 映射到一个词表长度的向量，向量的每一维对应一个 token；
- 缺点：向量维度过大、稀疏，不具备语义信息；

#### 词向量

将词映射到低维空间，使得相似的词具有相似的向量表示。

## Llama大模型结构

### RMS Normalization

为了防止梯度爆炸，对梯度进行标准化。

$$x_i=\frac{x_i\times g_i}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2+\epsilon}}$$

其中， $g$ 是权重向量， $\epsilon$ 是防止分母为 0 的小值。

### MLP

使用 Silu 作为激活函数。

$$Y=(Silu(X·W_{gate}^T)×X·W_{up}^T)·W_{down}^T$$

其中， $W_{gate}$ 、 $W_{up}$ 和 $W_{down}$ 是权重矩阵。

### 自注意力机制

每个 token 都会通过和权重矩阵 $(W_q, W_k, W_v)$ 相乘得到一组 query 向量 $Q_i$ ，key 向量 $K_i$ ，value 向量 $V_i$ 。

每个 token 的 query 都与之前 token 的 key 进行注意力计算，代表之前的某个词与当前词的关联程度（注意力）。

query 向量和 key 向量会额外计算旋转位置编码（RoPE）增加位置信息。

$$Attention(Q,K,V)=masked\underline{~}softmax(\frac{QK^T}{\sqrt{d_k}})V$$

最终结果与输出权重矩阵 $W_o$ 相乘得到维度为 hidden_size 的输出向量。

### 输出层

将输出向量映射到 token 空间，得到一个与词表相同长度的向量，代表下个 token 的概率分布。

一些模型的输出嵌入权重与输入嵌入权重共享（tie_word_embeddings）。
