# 大模型推理优化（一）

## AI对话服务的基本功能

1. 创建/切换/删除会话
2. 推理/中断推理
3. 修改历史对话重推理
4. 切换历史对话

## 服务多用户

### 静态批次的局限

- 对长度不同的输入进行padding会导致计算浪费
- 推理长度不同的请求造成额外的等待
- 不方便随时取消请求

### 连续批次（Orca）

- 以单轮推理为单位将请求池中的多个请求（可以长度不同）组成批次
- 可以批量计算的算子（如QKV计算）同时进行计算，不能批量计算的算子（如attention）每个请求单独进行计算

## 混合精度优化

模型的参数或计算使用更低精度的数据类型，从而达到减少内存使用或加速计算的目的。

### 大模型推理常见数据浮点数类型

- FP16：大小是FP32的一半，范围和精度都比FP32更低
- BF16：大小是FP32的一半，范围和FP32相同，但是精度比FP16更低
- TF32：大小和范围与FP32相同，精度和FP16相同，需要TensorCore支持

### 量化

- 将模型参数进一步压缩为更小的整数类型，如INT8
- 常用于手机之类内存有限的端侧设备
- 量化方法很多，主要目的是降低内存占用，不一定能提升计算速度

## 模型结构优化

### MAQ和GQA

多组Q对应一组KV，从而减少KV的大小

### 混合专家（MoE）模型

将MLP层替换为专家网络，推理时只激活部分专家
